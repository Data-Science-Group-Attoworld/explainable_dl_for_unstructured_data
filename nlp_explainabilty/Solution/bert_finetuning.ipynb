{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9006b7-356f-4255-bffd-f4f2c636c2b0",
   "metadata": {},
   "source": [
    "# Finetuning BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ea564-83ed-496a-ae29-f53677b23692",
   "metadata": {},
   "source": [
    "First, let's load the required libraries. We will use the popular transformers package for the model and the necessary preprocessing steps.\n",
    "\n",
    "Our goal in this notebook will be to classify sentences from the clinical context as to whether or not a medical condition is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52506e90-3661-48a1-9cc6-937978eec748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff1fc7-ffd0-45bc-8a84-42bedbeae500",
   "metadata": {},
   "source": [
    "We start by loading the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5faba2-df7b-4822-ae17-cfa0dcb4aded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f023536d9d4d738b7355173f2e5079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zita.zarandy\\AppData\\Local\\anaconda3\\envs\\biomed_data_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zita.zarandy\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f181fe355f408685d0bc1612a91ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e433a4e0eb4be7971b31375fb96a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f10054361354d3fbaf6e517ad3ecb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793868d43f124fd78d5b51e312ae91d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b2920-82fa-4ef5-a79e-83a101104ec7",
   "metadata": {},
   "source": [
    "The warning tells us that we need to train the model for our downstream task, as an untrained classification layer got added to the pretrained model.\n",
    "To see the effects of the fine tuning, we try to classify a \"medical\" sentence before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51994979-595c-470d-abf3-a905c94adb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do!:\n",
    "# Come up with two test sentences.\n",
    "# One should indicate the presence of a medical condition, and the other should come from a clinical setting but not indicate any medical condition.\n",
    "\n",
    "test_sentence_condition = \"The patient suffers from chronic headaches.\"\n",
    "test_sentence_no_condition = \"The hospital has a new meal plan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9757e3c3-7377-4cff-8171-d9a37da18f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'The hospital has a new meal plan.'\n",
      "Logits: [-0.22869185  0.11311549]\n",
      "Probabilities (No Condition, Medical Condition): [0.41537052 0.5846295 ]\n",
      "Predicted Class (before training): 1\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_to_classify = test_sentence_no_condition\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tokenize the sentence and get tensors\n",
    "encoding = tokenizer(sentence_to_classify, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(f\"Sentence: '{sentence_to_classify}'\")\n",
    "print(f\"Logits: {logits.cpu().numpy()[0]}\")\n",
    "print(\n",
    "    f\"Probabilities (No Condition, Medical Condition): {probabilities.cpu().numpy()[0]}\"\n",
    ")\n",
    "print(f\"Predicted Class (before training): {predicted_class}\")\n",
    "print(\"--------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e070b5b-d9f7-4211-83e9-793744991618",
   "metadata": {},
   "source": [
    "## **Finetuning the model**\n",
    "\n",
    "Let's load some data and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e9f598-4d0e-408a-97bb-0cca6114b725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>Undergoing chemotherapy for lymphoma.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>Discharged from the hospital this morning.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Surgical intervention is recommended.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Prescribed medication for high cholesterol.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>Medical history was updated in the file.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         text  label\n",
       "0          19        Undergoing chemotherapy for lymphoma.      1\n",
       "1          28   Discharged from the hospital this morning.      0\n",
       "2           8        Surgical intervention is recommended.      1\n",
       "3           7  Prescribed medication for high cholesterol.      1\n",
       "4          27     Medical history was updated in the file.      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Do!\n",
    "\n",
    "# 1. Load the file medical_data.csv as pandas dataframe\n",
    "# 2. Display the content of the dataframe. How is the label information encoded?\n",
    "# 3. Load the content of the column \"text\" into a list called texts and the column label into a list called labels\n",
    "\n",
    "df = pd.read_csv(\"../medical_data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "572aa287-7857-49e6-ace4-43fb2443a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(df[\"text\"])\n",
    "labels = list(df[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31af87-815d-4f73-8f98-4f7d1cadba78",
   "metadata": {},
   "source": [
    "Let's create training and validation splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854a0ebb-9ecd-4170-9258-9cefc9032bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07de09a-7050-4992-8a84-6405b18d72da",
   "metadata": {},
   "source": [
    "Bring data in the right format;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca306a98-5e91-4529-ae6c-b4789ccff50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 Training samples.\n",
      "6 Validation samples.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert tokenized inputs to PyTorch tensors\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings[\"input_ids\"]),\n",
    "    torch.tensor(train_encodings[\"attention_mask\"]),\n",
    "    torch.tensor(train_labels),\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_encodings[\"input_ids\"]),\n",
    "    torch.tensor(val_encodings[\"attention_mask\"]),\n",
    "    torch.tensor(val_labels),\n",
    ")\n",
    "\n",
    "print(f\"{len(train_dataset)} Training samples.\")\n",
    "print(f\"{len(val_dataset)} Validation samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38ba0a-c4c2-4a8c-ad6d-6ff0ca9295c9",
   "metadata": {},
   "source": [
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56f170b-2da7-4dcf-9d7e-2074dd4ba795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  1996, 16012, 18075,  3463,  2020,  3893,  2005, 16007, 28207,\n",
       "          5666,  1012,   102,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5ea2a1-4b8f-42ff-bc83-ff38fa221377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 2668, 3778, 2003, 2306, 1996, 3671, 2846, 1012,  102,    0,    0,\n",
       "            0,    0,    0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb549ed3-85ba-4edf-a21a-853203a650a2",
   "metadata": {},
   "source": [
    "Next, we create our dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d353b1-ce9a-460b-9f69-a28a1c9cd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c3be9-b9b3-425e-989c-d9bc8c4475aa",
   "metadata": {},
   "source": [
    "Finally, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45e6199-2323-4630-b3e5-30923b72ebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.7435\n",
      "Validating...\n",
      "  Validation Loss: 0.6835\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.6547\n",
      "Validating...\n",
      "  Validation Loss: 0.6718\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.6248\n",
      "Validating...\n",
      "  Validation Loss: 0.6447\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.5266\n",
      "Validating...\n",
      "  Validation Loss: 0.5554\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.3538\n",
      "Validating...\n",
      "  Validation Loss: 0.4841\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.2011\n",
      "Validating...\n",
      "  Validation Loss: 0.4793\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.1247\n",
      "Validating...\n",
      "  Validation Loss: 0.4920\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.0948\n",
      "Validating...\n",
      "  Validation Loss: 0.4992\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.0800\n",
      "Validating...\n",
      "  Validation Loss: 0.5061\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Average training loss: 0.0778\n",
      "Validating...\n",
      "  Validation Loss: 0.5081\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "\n",
    "# Create a learning rate scheduler to linearly decrease the learning rate over the training epochs\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n======== Epoch {epoch + 1} / {epochs} ========\")\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # move data to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    print(\"Validating...\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "\n",
    "            # move to device\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be1224-62be-4d22-a813-06413ccab15f",
   "metadata": {},
   "source": [
    "Let's redo the classification of our test sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42849f27-c949-4779-86ff-05c001401db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'The patient suffers from chronic headaches.'\n",
      "Logits: [-1.0838794  1.3528422]\n",
      "Probabilities (No Condition, Medical Condition): [0.08041502 0.919585  ]\n",
      "Predicted Class (after training): 1\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To Do!\n",
    "\n",
    "# Redo the classification on your test sentences!\n",
    "\n",
    "sentence_to_classify = test_sentence_condition\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tokenize the sentence and get tensors\n",
    "encoding = tokenizer(sentence_to_classify, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(f\"Sentence: '{sentence_to_classify}'\")\n",
    "print(f\"Logits: {logits.cpu().numpy()[0]}\")\n",
    "print(\n",
    "    f\"Probabilities (No Condition, Medical Condition): {probabilities.cpu().numpy()[0]}\"\n",
    ")\n",
    "print(f\"Predicted Class (after training): {predicted_class}\")\n",
    "print(\"--------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5987f-5e44-44be-bb38-1f48aafce752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
