{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9006b7-356f-4255-bffd-f4f2c636c2b0",
   "metadata": {},
   "source": [
    "# Finetuning BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ea564-83ed-496a-ae29-f53677b23692",
   "metadata": {},
   "source": [
    "First, let's load the required libraries. We will use the popular transformers package for the model and the necessary preprocessing steps.\n",
    "\n",
    "Our goal in this notebook will be to classify sentences from the clinical context as to whether or not a medical condition is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52506e90-3661-48a1-9cc6-937978eec748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff1fc7-ffd0-45bc-8a84-42bedbeae500",
   "metadata": {},
   "source": [
    "We start by loading the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5faba2-df7b-4822-ae17-cfa0dcb4aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b2920-82fa-4ef5-a79e-83a101104ec7",
   "metadata": {},
   "source": [
    "The warning tells us that we need to train the model for our downstream task, as an untrained classification layer got added to the pretrained model.\n",
    "To see the effects of the fine tuning, we try to classify a \"medical\" sentence before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51994979-595c-470d-abf3-a905c94adb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do!:\n",
    "# Come up with two test sentences.\n",
    "# One should indicate the presence of a medical condition, and the other should come from a clinical setting but not indicate any medical condition.\n",
    "\n",
    "test_sentence_condition = \"\"\n",
    "test_sentence_no_condition = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757e3c3-7377-4cff-8171-d9a37da18f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_classify = test_sentence_condition\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tokenize the sentence and get tensors\n",
    "encoding = tokenizer(sentence_to_classify, return_tensors='pt')\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(f\"Sentence: '{sentence_to_classify}'\")\n",
    "print(f\"Logits: {logits.cpu().numpy()[0]}\")\n",
    "print(f\"Probabilities (No Condition, Medical Condition): {probabilities.cpu().numpy()[0]}\")\n",
    "print(f\"Predicted Class (before training): {predicted_class}\")\n",
    "print(\"--------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e070b5b-d9f7-4211-83e9-793744991618",
   "metadata": {},
   "source": [
    "## **Finetuning the model**\n",
    "\n",
    "Let's load some data and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17e9f598-4d0e-408a-97bb-0cca6114b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do!\n",
    "\n",
    "# 1. Load the file medical_data.csv as pandas dataframe\n",
    "# 2. Display the content of the dataframe. How is the label information encoded?\n",
    "# 3. Load the content of the column \"text\" into a list called texts and the column label into a list called labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31af87-815d-4f73-8f98-4f7d1cadba78",
   "metadata": {},
   "source": [
    "Let's create training and validation splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "854a0ebb-9ecd-4170-9258-9cefc9032bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07de09a-7050-4992-8a84-6405b18d72da",
   "metadata": {},
   "source": [
    "Bring data in the right format;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca306a98-5e91-4529-ae6c-b4789ccff50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert tokenized inputs to PyTorch tensors\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings[\"input_ids\"]),\n",
    "    torch.tensor(train_encodings[\"attention_mask\"]),\n",
    "    torch.tensor(train_labels)\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_encodings[\"input_ids\"]),\n",
    "    torch.tensor(val_encodings[\"attention_mask\"]),\n",
    "    torch.tensor(val_labels)\n",
    ")\n",
    "\n",
    "print(f\"{len(train_dataset)} Training samples.\")\n",
    "print(f\"{len(val_dataset)} Validation samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38ba0a-c4c2-4a8c-ad6d-6ff0ca9295c9",
   "metadata": {},
   "source": [
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f170b-2da7-4dcf-9d7e-2074dd4ba795",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ea2a1-4b8f-42ff-bc83-ff38fa221377",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb549ed3-85ba-4edf-a21a-853203a650a2",
   "metadata": {},
   "source": [
    "Next, we create our dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0d353b1-ce9a-460b-9f69-a28a1c9cd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c3be9-b9b3-425e-989c-d9bc8c4475aa",
   "metadata": {},
   "source": [
    "Finally, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e6199-2323-4630-b3e5-30923b72ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "\n",
    "# Create a learning rate scheduler to linearly decrease the learning rate over the training epochs\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n======== Epoch {epoch + 1} / {epochs} ========\")\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # move data to device\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    print(\"Validating...\")\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  s\n",
    "        for batch in val_loader:\n",
    "\n",
    "            # move to device\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be1224-62be-4d22-a813-06413ccab15f",
   "metadata": {},
   "source": [
    "Let's redo the classification of our test sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42849f27-c949-4779-86ff-05c001401db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do!\n",
    "\n",
    "# Redo the classification on your test sentences!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:computer_vision]",
   "language": "python",
   "name": "conda-env-computer_vision-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
