{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72b847ab-5372-4f99-8529-98888a2e5c8a",
   "metadata": {},
   "source": [
    "# Lime: \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\n",
    "\n",
    "The first explainability algorithm we want to look at is Lime (https://arxiv.org/abs/1602.04938). \n",
    "\n",
    "We will use the dedicated python library lime for our analyis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e767e7-42d6-41d2-8e00-6f4a8b61b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports again\n",
    "from lime import lime_image\n",
    "import numpy as np\n",
    "import medmnist\n",
    "from skimage.segmentation import slic\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009dde8-792b-4f86-a97b-4506384c3334",
   "metadata": {},
   "source": [
    "Let's start with loading the petrained classifier model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d659d-4e59-43f6-82ae-5d78cbcdff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18()\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 7)\n",
    "model.load_state_dict(torch.load(\"best_dermamnist_resnet_model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3f164-0759-4b5d-8a02-ecfc995a4833",
   "metadata": {},
   "source": [
    "Then, just as in the model training notebook, we get test data that we want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbaaa8f-128b-4a30-89d6-edef1fc4cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'dermamnist'\n",
    "info = medmnist.INFO[data_flag]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "batch_size = 128\n",
    "\n",
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "test_dataset = DataClass(split='test', transform=data_transform, size=64, download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be22f88-edae-45ad-9347-0e5b3da1d4c0",
   "metadata": {},
   "source": [
    "Get the first batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11211c-1504-4e26-81f5-8e6cf32a2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b92e60-9f58-4fdb-a4e6-a390edbd5295",
   "metadata": {},
   "source": [
    "## **Applying Lime**\n",
    "\n",
    "To understand why our classifier behaves the way it does, we can use LimeImageExplainer().\n",
    "LimeImageExplainer requires two inputs. First, we need to provide an image that we want to analyze. Second, we need to provide a function that can be used to predict the perturbed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe3331-55a7-4e38-9c93-9f49763fdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To DO\n",
    "# 1. Select an image from images.\n",
    "# 2. Lime expects input images with HeightxWidthx3, with the last dimension beeing the RGB channels.\n",
    "# Our images are currently in the form of 3xHxW. We need to permute the dimensions in the right order.\n",
    "# 3. Convert tensor to numpy array and name image img_np.\n",
    "# 4. Undo normalization step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4941a67-6183-4a28-9b1c-2fb5c2c3210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do\n",
    "# Complete the function\n",
    "\n",
    "def batch_predict(images):\n",
    "  \n",
    "    # 1. Set model in evaluation mode\n",
    "    # 2. Repermute image dimensions to align with the model\n",
    "    # 3. Do normalization\n",
    "    # 4. Predict on images\n",
    "    # 5. Apply softmax function and detach output\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becaa3ea-7554-49dd-975f-e0e8eac7f950",
   "metadata": {},
   "source": [
    "Then we can bring everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79dcca-0c37-4f7d-b444-ba2061cba089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom segmentation map, because of image size.\n",
    "segmentation_fn = lambda x: slic(x, n_segments=30, compactness=10, sigma=1, start_label=1)\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(img_np,\n",
    "                                         batch_predict,\n",
    "                                         top_labels=3, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000,\n",
    "                                         segmentation_fn=segmentation_fn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d32d23-cc53-4898-badd-39891bba306a",
   "metadata": {},
   "source": [
    "Let's have a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581d043-d7a2-4a7d-ac2a-7e45bd9b6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=3, hide_rest=False)\n",
    "img_boundry2 = mark_boundaries(temp, mask)\n",
    "\n",
    "\n",
    "plt.imshow(np.clip(img_boundry2, 0, 1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:computer_vision]",
   "language": "python",
   "name": "conda-env-computer_vision-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
